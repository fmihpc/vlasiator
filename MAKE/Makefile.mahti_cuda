# Suggested modules
# This is updated for Mahti RHEL8 as of 18th May 2022

# module purge
# module load StdEnv
# module load gcc/9.4.0
# module load cuda/11.5.0
# module load openmpi/4.1.2-cuda
# module load jemalloc
# module load papi
# one-liner:
# module purge; module load StdEnv; module load gcc/9.4.0; module load cuda/11.5.0; module load openmpi/4.1.2-cuda; module load jemalloc; module load papi

#  Also, add this to your job script!
#export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/projappl/project_2000203/libraries_rhel8_gcccuda/boost/lib

#======== Vectorization ==========
#Set vector backend type for vlasov solvers, sets precision and length.
#Options:
# AVX:	    VEC4D_AGNER, VEC4F_AGNER, VEC8F_AGNER
# AVX512:   VEC8D_AGNER, VEC16F_AGNER
# Fallback: VECTORCLASS = VEC_FALLBACK_GENERIC (Defaults to VECL8)
VECTORCLASS = VEC_FALLBACK_GENERIC

#===== Vector Lenghts ====
# Default for VEC_FALLBACK_GENERIC is WID=4, VECL=8
# NOTE: A bug currently results in garbage data already on cell init if VECL is not equal to WID2
#WID=8
#VECL=64
WID=4
VECL=16

#======= Compiler and compilation flags =========
# NOTES on compiler flags:
# CXXFLAGS is for compiler flags, they are always used
# MATHFLAGS are for special math etc. flags, these are only applied on solver functions
# LDFLAGS flags for linker
# Important note: Do not edit COMPFLAGS in this file!

#-DNO_WRITE_AT_ALL:  Define to disable write at all to
#                    avoid memleak (much slower IO)
#-DMPICH_IGNORE_CXX_SEEK: Ignores some multiple definition
#                         errors that come up when using
#                         mpi.h in c++ on Cray

USE_CUDA=1
CUDABLOCKS=108

#-ggdb not available on nvcc
#-G (device debug) overrides --generate-line-info -line-info but also requires more device-side resources to run
# use "-Xptxas -v" for verbose output of ptx compilation

CXXFLAGS = -g -O3 -x cu -std=c++17 --extended-lambda --expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 --generate-line-info -line-info -Xcompiler="-fopenmp" -Xcompiler="-fpermissive" -maxrregcount 24
testpackage: CXXFLAGS = -g -O2 -x cu -std=c++17 --extended-lambda --expt-relaxed-constexpr -gencode arch=compute_80,code=sm_80 --generate-line-info -line-info -Xcompiler="-fopenmp" -Xcompiler="-fpermissive" -maxrregcount 24

# Tell mpic++ to use nvcc for all compiling
CMP = OMPI_CXX='nvcc' OMPI_CXXFLAGS='' mpic++

# Now tell also the linker to use nvcc
## The line below indeed uses OMPI_CXX, not OMPI_LD
LNK = OMPI_CXX='nvcc' OMPI_CXXFLAGS='-arch=sm_80' OMPI_LIBS='-L/appl/spack/v017/install-tree/gcc-9.4.0/openmpi-4.1.2-cgr4nz/lib -L/appl/spack/syslibs/lib' OMPI_LDFLAGS=' -Xlinker=-rpath=/appl/spack/v017/install-tree/gcc-8.5.0/gcc-9.4.0-nfm4wc/lib/gcc/x86_64-pc-linux-gnu/9.4.0 -Xlinker=-rpath=/appl/spack/v017/install-tree/gcc-8.5.0/gcc-9.4.0-nfm4wc/lib64 -Xlinker=-rpath=/appl/spack/v017/install-tree/gcc-9.4.0/openmpi-4.1.2-cgr4nz -Xlinker=-rpath=/lib/appl/spack/syslibs/lib -lmpi ' mpic++

MATHFLAGS = --use_fast_math
# nvcc fast_math does not assume only finite math
testpackage: MATHFLAGS = --prec-sqrt=true --prec-div=true --ftz=false --fmad=false

LDFLAGS = -O2 -g -G -L/appl/spack/v016/install-tree/gcc-4.8.5/nvhpc-21.2-l6xyb4/Linux_x86_64/21.2/cuda/11.2/lib64 -lnvToolsExt
LIB_MPI = -lgomp

LIB_CUDA = -L/usr/local/cuda/lib64
INC_CUDA = -isystem /usr/local/cuda/include

#======== PAPI ==========
#Add PAPI_MEM define to use papi to report memory consumption?
CXXFLAGS += -DPAPI_MEM
testpackage: CXXFLAGS += -DPAPI_MEM

#======== Jemalloc =========
#Use jemalloc instead of system malloc to reduce memory fragmentation https://github.com/jemalloc/jemalloc
#Configure jemalloc with  --with-jemalloc-prefix=je_ when installing it
#NOTE: jemalloc not supported with GPUs
#CXXFLAGS += -DUSE_JEMALLOC -DJEMALLOC_NO_DEMANGLE
#testpackage: CXXFLAGS += -DUSE_JEMALLOC -DJEMALLOC_NO_DEMANGLE

#======== Umpire =========
CXXFLAGS += -DUSE_UMPIRE
testpackage: CXXFLAGS += -DUSE_UMPIRE

#======== Libraries ===========

LIBRARY_PREFIX = /projappl/project_2002873/libraries/nvhpc/21.2
LIBRARY_PREFIX2 = /projappl/project_2000203/libraries_rhel8_gcccuda

#======== Compiled Libraries ===========

INC_BOOST = -isystem /$(LIBRARY_PREFIX2)/boost/include
LIB_BOOST = -L/$(LIBRARY_PREFIX2)/boost/lib -lboost_program_options

# module
LIB_PAPI = -lpapi
#INC_PAPI = -isystem /$(LIBRARY_PREFIX2)/papi/include
#LIB_PAPI = -L/$(LIBRARY_PREFIX2)/papi/lib -lpapi

INC_ZOLTAN = -isystem /$(LIBRARY_PREFIX2)/zoltan/include
LIB_ZOLTAN = -L/$(LIBRARY_PREFIX2)/zoltan/lib -lzoltan

# module
#LIB_JEMALLOC = -ljemalloc
#INC_JEMALLOC = -isystem /$(LIBRARY_PREFIX2)/jemalloc/include
#LIB_JEMALLOC = -L/$(LIBRARY_PREFIX2)/jemalloc/lib -ljemalloc -Xlinker=-rpath=$(LIBRARY_PREFIX2)/jemalloc/lib
#LIB_JEMALLOC = -L/$(LIBRARY_PREFIX2)/jemalloc/lib -ljemalloc -Wl,-rpath=$(LIBRARY_PREFIX2)/jemalloc/lib

INC_VLSV = -I$(LIBRARY_PREFIX2)/vlsv
LIB_VLSV = -L$(LIBRARY_PREFIX2)/vlsv -lvlsv -Xlinker=-rpath=$(LIBRARY_PREFIX2)/vlsv/lib
#LIB_VLSV = -L$(LIBRARY_PREFIX2)/vlsv -lvlsv -Wl,-rpath=$(LIBRARY_PREFIX2)/vlsv/lib

INC_PROFILE = -I$(LIBRARY_PREFIX2)/phiprof_nvcc/include
LIB_PROFILE = -L$(LIBRARY_PREFIX2)/phiprof_nvcc/lib -lphiprof -Xlinker=-rpath=$(LIBRARY_PREFIX2)/phiprof_nvcc/lib
#LIB_PROFILE = -L$(LIBRARY_PREFIX2)/phiprof_nvcc/lib -lphiprof -Wl,-rpath=$(LIBRARY_PREFIX2)/phiprof_nvcc/lib

INC_UMPIRE = -isystem $(LIBRARY_PREFIX2)/umpire/include
LIB_UMPIRE = -L$(LIBRARY_PREFIX2)/umpire/lib -L$(LIBRARY_PREFIX)/umpire/lib64 -lcamp -lumpire -lfmt

#======== Header-only Libraries ===========

INC_EIGEN = -isystem $(LIBRARY_PREFIX)/eigen
INC_FSGRID = -I./submodules/fsgrid
INC_DCCRG = -I./submodules/dccrg
# Vectorclass only for CPU mode
#INC_VECTORCLASS = -I ./submodules/vectorclass/ -I ./submodules/vectorclass-addon/vector3d/
# Hashinator not yet as a submodule
INC_HASHINATOR = -I$(HOME)/git/hashinator/
