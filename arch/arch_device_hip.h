
#ifndef ARCH_DEVICE_HIP_H
#define ARCH_DEVICE_HIP_H

/* Include required headers */
#include <hip/hip_runtime.h>
#include <hipcub/device/device_radix_sort.hpp>
#include <hipcub/hipcub.hpp>

#include <omp.h>

#if defined(USE_UMPIRE)
#include "umpire/Allocator.hpp"
#include "umpire/ResourceManager.hpp"
#include "umpire/strategy/QuickPool.hpp"
#endif

/* architecture-agnostic definitions for HIP */

#define gpuGetLastError hipGetLastError
#define gpuGetErrorString hipGetErrorString
#define gpuPeekAtLastError hipPeekAtLastError

#define gpuSetDevice hipSetDevice
#define gpuGetDevice hipGetDevice
#define gpuGetDeviceCount hipGetDeviceCount
#define gpuGetDeviceProperties hipGetDeviceProperties
#define gpuDeviceSynchronize hipDeviceSynchronize
#define gpuDeviceReset hipDeviceReset
#define gpuCpuDeviceId hipCpuDeviceId

#if defined(USE_UMPIRE)
static hipError_t gpuMalloc(void** dev_ptr, size_t size, hipStream_t stream = 0) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("DEV_POOL");
   void* umpire_ptr = static_cast<void*>(allocator.allocate(size));
   if (umpire_ptr != nullptr) {
      *dev_ptr = umpire_ptr;
      return hipSuccess;
   } else {
      return hipErrorMemoryAllocation;
   }
}
static hipError_t gpuFree(void* dev_ptr, hipStream_t stream = 0) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("DEV_POOL");
   allocator.deallocate(dev_ptr);
   return hipSuccess;
}
#define gpuMallocAsync gpuMalloc
#define gpuFreeAsync gpuFree
static hipError_t gpuMallocHost(void** pinned_ptr, size_t size) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("PINNED_POOL");
   void* umpire_ptr = static_cast<void*>(allocator.allocate(size));
   if (umpire_ptr != nullptr) {
      *pinned_ptr = umpire_ptr;
      return hipSuccess;
   } else {
      return hipErrorMemoryAllocation;
   }
}
static hipError_t gpuFreeHost(void* pinned_ptr) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("PINNED_POOL");
   allocator.deallocate(pinned_ptr);
   return hipSuccess;
}
static hipError_t gpuMallocManaged(void** dev_ptr, size_t size) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("UM_POOL");
   void* umpire_ptr = static_cast<void*>(allocator.allocate(size));
   if (umpire_ptr != nullptr) {
      *dev_ptr = umpire_ptr;
      return hipSuccess;
   } else {
      return hipErrorMemoryAllocation;
   }
}
static hipError_t gpuFreeManaged(void* dev_ptr) {
   auto& rm = umpire::ResourceManager::getInstance();
   auto allocator = rm.getAllocator("UM_POOL");
   allocator.deallocate(dev_ptr);
   return hipSuccess;
}
#else
#define gpuMalloc hipMalloc
#define gpuFree hipFree
#define gpuMallocAsync hipMallocAsync
#define gpuFreeAsync hipFreeAsync
#define gpuMallocHost hipMallocHost
#define gpuFreeHost hipFreeHost
#define gpuMallocManaged hipMallocManaged
#define gpuFreeManaged hipFree
#endif
// empty comment //
#define gpuHostAlloc hipHostMalloc
#define gpuHostAllocPortable hipHostAllocPortable
#define gpuMemcpy hipMemcpy
#define gpuMemcpyAsync hipMemcpyAsync
#define gpuMemset hipMemset
#define gpuMemsetAsync hipMemsetAsync

#define gpuMemAdviseSetAccessedBy hipMemAdviseSetAccessedBy
#define gpuMemAdviseSetPreferredLocation hipMemAdviseSetPreferredLocation
#define gpuMemAttachSingle hipMemAttachSingle
#define gpuMemAttachGlobal hipMemAttachGlobal
#define gpuMemPrefetchAsync hipMemPrefetchAsync

#define gpuStreamCreate hipStreamCreate
#define gpuHostRegister hipHostRegister
#define gpuHostRegisterPortable hipHostRegisterPortable

#define gpuStreamDestroy hipStreamDestroy
#define gpuStreamWaitEvent hipStreamWaitEvent
#define gpuStreamSynchronize hipStreamSynchronize
#define gpuStreamAttachMemAsync hipStreamAttachMemAsync
#define gpuDeviceGetStreamPriorityRange hipDeviceGetStreamPriorityRange
#define gpuStreamCreateWithPriority hipStreamCreateWithPriority
#define gpuStreamDefault hipStreamDefault

#define gpuEventCreate hipEventCreate
#define gpuEventCreateWithFlags hipEventCreateWithFlags
#define gpuEventDestroy hipEventDestroy
#define gpuEventQuery hipEventQuery
#define gpuEventRecord hipEventRecord
#define gpuEventSynchronize hipEventSynchronize
#define gpuEventElapsedTime hipEventElapsedTime

/* driver_types */
#define gpuError_t hipError_t
#define gpuSuccess hipSuccess

#define gpuStream_t hipStream_t
#define gpuDeviceProp hipDeviceProp_t

#define gpuEvent_t hipEvent_t
#define gpuEventDefault hipEventDefault
#define gpuEventBlockingSync hipEventBlockingSync
#define gpuEventDisableTiming hipEventDisableTiming

#define gpuMemcpyKind hipMemcpyKind
#define gpuMemcpyDeviceToHost hipMemcpyDeviceToHost
#define gpuMemcpyHostToDevice hipMemcpyHostToDevice
#define gpuMemcpyDeviceToDevice hipMemcpyDeviceToDevice
#define gpuMemcpyToSymbol hipMemcpyToSymbol

#define gpuKernelBallot(mask, input) __ballot(input)
#define gpuKernelAny(mask, input) __any(input)

/* Define architecture-specific macros */
#define ARCH_LOOP_LAMBDA [=] __host__ __device__
#define ARCH_INNER_BODY2(i, j, aggregate)
#define ARCH_INNER_BODY3(i, j, k, aggregate)
#define ARCH_INNER_BODY4(i, j, k, l, aggregate)

/* Ensure printing of CUB GPU runtime errors to console */
#define HIPCUB_STDERR

/* Set HIP blocksize used for reductions */
#define ARCH_BLOCKSIZE_R 512
#define ARCH_BLOCKSIZE_R_SMALL 64

/* values used by kernels */
#ifndef GPUTHREADS
#define GPUTHREADS (64)
#endif
#ifndef WARPSPERBLOCK
#define WARPSPERBLOCK (16)
#endif
#define FULL_MASK 0xffffffffffffffff

extern hipStream_t gpuStreamList[];

/* Define the HIP error checking macro */
#define CHK_ERR(err) (hip_error(err, __FILE__, __LINE__))
inline static void hip_error(hipError_t err, const char* file, int line) {
   if (err != hipSuccess) {
      printf("\n\n%s in %s at line %d\n", hipGetErrorString(err), file, line);
      exit(1);
   }
}

/* Namespace for architecture-specific functions */
namespace arch {

/* Create auxiliary max atomic function for double types */
__device__ __forceinline__ static void atomicMax(double* address, double val2) {
   unsigned long long ret = __double_as_longlong(*address);
   while (val2 > __longlong_as_double(ret)) {
      unsigned long long old = ret;
      if ((ret = atomicCAS((unsigned long long*)address, old, __double_as_longlong(val2))) == old)
         break;
   }
}

/* Create auxiliary min atomic function for double types */
__device__ __forceinline__ static void atomicMin(double* address, double val2) {
   unsigned long long ret = __double_as_longlong(*address);
   while (val2 < __longlong_as_double(ret)) {
      unsigned long long old = ret;
      if ((ret = atomicCAS((unsigned long long*)address, old, __double_as_longlong(val2))) == old)
         break;
   }
}

/* Buffer class for making data available on device */
template <typename T> class buf {
private:
   T* ptr;
   T* d_ptr;
   uint bytes;
   uint is_copy = 0;
   uint thread_id = 0;

public:
   void syncDeviceData(void) {
      CHK_ERR(hipMemcpyAsync(d_ptr, ptr, bytes, hipMemcpyHostToDevice, gpuStreamList[thread_id]));
   }

   void syncHostData(void) {
      CHK_ERR(hipMemcpyAsync(ptr, d_ptr, bytes, hipMemcpyDeviceToHost, gpuStreamList[thread_id]));
   }

   buf(T* const _ptr, uint _bytes) : ptr(_ptr), bytes(_bytes) {
      thread_id = omp_get_thread_num();
      CHK_ERR(hipMallocAsync(&d_ptr, bytes, gpuStreamList[thread_id]));
      syncDeviceData();
   }

   __host__ __device__ buf(const buf& u)
       : ptr(u.ptr), d_ptr(u.d_ptr), bytes(u.bytes), is_copy(1), thread_id(u.thread_id) {}

   __host__ ~buf(void) {
      if (!is_copy) {
// syncHostData();
#ifdef __HIP_DEVICE_COMPILE__
         hipFreeAsync(d_ptr, gpuStreamList[thread_id]);
#endif
      }
   }

   __host__ __device__ T& operator[](uint i) const {
#ifdef __HIP_DEVICE_COMPILE__
      return d_ptr[i];
#else
      return ptr[i];
#endif
   }
};

/* A function to check and set the device mempool settings */
__host__ __forceinline__ static void device_mempool_check(uint64_t threshold_new) {
   int device_id;
   CHK_ERR(hipGetDevice(&device_id));
   hipMemPool_t mempool;
   CHK_ERR(hipDeviceGetDefaultMemPool(&mempool, device_id));
   uint64_t threshold_old;
   CHK_ERR(hipMemPoolGetAttribute(mempool, hipMemPoolAttrReleaseThreshold, &threshold_old));
   if (threshold_new != threshold_old)
      CHK_ERR(hipMemPoolSetAttribute(mempool, hipMemPoolAttrReleaseThreshold, &threshold_new));
}

/* Device function for memory allocation */
__host__ __forceinline__ static void* allocate(size_t bytes) {
   void* ptr;
   const uint thread_id = omp_get_thread_num();
   device_mempool_check(UINT64_MAX);
   CHK_ERR(hipMallocAsync(&ptr, bytes, gpuStreamList[thread_id]));
   return ptr;
}

__host__ __forceinline__ static void* allocate(size_t bytes, hipStream_t stream) {
   void* ptr;
   device_mempool_check(UINT64_MAX);
   CHK_ERR(hipMallocAsync(&ptr, bytes, stream));
   return ptr;
}

/* Device function for memory deallocation */
template <typename T> __host__ __forceinline__ static void free(T* ptr) {
   const uint thread_id = omp_get_thread_num();
   CHK_ERR(hipFreeAsync(ptr, gpuStreamList[thread_id]));
}

template <typename T> __host__ __forceinline__ static void free(T* ptr, hipStream_t stream) {
   CHK_ERR(hipFreeAsync(ptr, stream));
}
/* Host-to-device memory copy */
template <typename T> __forceinline__ static void memcpy_h2d(T* dst, T* src, size_t bytes) {
   const uint thread_id = omp_get_thread_num();
   CHK_ERR(hipMemcpyAsync(dst, src, bytes, hipMemcpyHostToDevice, gpuStreamList[thread_id]));
}

template <typename T> __forceinline__ static void memcpy_h2d(T* dst, T* src, size_t bytes, hipStream_t stream) {
   CHK_ERR(hipMemcpyAsync(dst, src, bytes, hipMemcpyHostToDevice, stream));
}

/* Device-to-host memory copy */
template <typename T> __forceinline__ static void memcpy_d2h(T* dst, T* src, size_t bytes) {
   const uint thread_id = omp_get_thread_num();
   CHK_ERR(hipMemcpyAsync(dst, src, bytes, hipMemcpyDeviceToHost, gpuStreamList[thread_id]));
}

template <typename T> __forceinline__ static void memcpy_d2h(T* dst, T* src, size_t bytes, hipStream_t stream) {
   CHK_ERR(hipMemcpyAsync(dst, src, bytes, hipMemcpyDeviceToHost, stream));
}

/* Register, ie, page-lock existing host allocations */
template <typename T> __forceinline__ static void host_register(T* ptr, size_t bytes) {
   CHK_ERR(hipHostRegister(ptr, bytes, hipHostRegisterDefault));
}

/* Unregister page-locked host allocations */
template <typename T> __forceinline__ static void host_unregister(T* ptr) { CHK_ERR(hipHostUnregister(ptr)); }

/* Specializations for lambda calls depending on the templated dimension */
template <typename Lambda, typename T>
__device__ __forceinline__ static void lambda_eval(const uint (&idx)[1], T* __restrict__ thread_data,
                                                   Lambda loop_body) {
   loop_body(idx[0], thread_data);
}

template <typename Lambda, typename T>
__device__ __forceinline__ static void lambda_eval(const uint (&idx)[2], T* __restrict__ thread_data,
                                                   Lambda loop_body) {
   loop_body(idx[0], idx[1], thread_data);
}

template <typename Lambda, typename T>
__device__ __forceinline__ static void lambda_eval(const uint (&idx)[3], T* __restrict__ thread_data,
                                                   Lambda loop_body) {
   loop_body(idx[0], idx[1], idx[2], thread_data);
}

template <typename Lambda, typename T>
__device__ __forceinline__ static void lambda_eval(const uint (&idx)[4], T* __restrict__ thread_data,
                                                   Lambda loop_body) {
   loop_body(idx[0], idx[1], idx[2], idx[3], thread_data);
}

/* Get the index for the underlying dimension, and call the respective lambda wrapper */
template <uint NDim, typename Lambda, typename T>
__device__ __forceinline__ static void loop_eval(const uint idx_glob, const uint* __restrict__ lims,
                                                 T* __restrict__ thread_data, Lambda loop_body) {
   uint idx[NDim];
   switch (NDim) {
   case 4:
      idx[3] = (idx_glob / (lims[0] * lims[1] * lims[2])) % lims[3];
   case 3:
      idx[2] = (idx_glob / (lims[0] * lims[1])) % lims[2];
   case 2:
      idx[1] = (idx_glob / lims[0]) % lims[1];
   case 1:
      idx[0] = idx_glob % lims[0];
   }
   lambda_eval(idx, thread_data, loop_body);
}

/* A general device kernel for reductions */
template <uint Blocksize, reduce_op Op, uint NDim, uint NReduStatic, typename Lambda, typename T>
__global__ static void __launch_bounds__(ARCH_BLOCKSIZE_R)
    reduction_kernel(Lambda loop_body, const T* __restrict__ init_val, T* __restrict__ rslt,
                     const uint* __restrict__ lims, const uint n_total, const uint n_redu_dynamic,
                     T* thread_data_dynamic) {
   /* Specialize BlockReduce for a 1D block of Blocksize threads of type `T` */
   typedef hipcub::BlockReduce<T, Blocksize, hipcub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY, 1, 1> BlockReduce;

   /* Dynamic shared memory declaration */
   extern __shared__ char temp_storage_dynamic[];

   /* Static shared memory declaration */
   constexpr uint size = NReduStatic ? NReduStatic : 1;
   __shared__ typename BlockReduce::TempStorage temp_storage_static[size];

   /* Assign a pointer to the shared memory (dynamic or static case) */
   typename BlockReduce::TempStorage* temp_storage =
       NReduStatic ? temp_storage_static : (typename BlockReduce::TempStorage*)temp_storage_dynamic;

   /* Get the global 1D thread index*/
   const uint idx_glob = blockIdx.x * blockDim.x + threadIdx.x;

   /* Static thread data declaration */
   T thread_data_static[size];

   /* Assign a pointer to the thread data (dynamic or static case)*/
   T* thread_data = NReduStatic ? thread_data_static : &thread_data_dynamic[n_redu_dynamic * idx_glob];

   /* Get the number of reductions (may be known at compile time or not) */
   const uint n_reductions = NReduStatic ? NReduStatic : n_redu_dynamic;

   /* Set initial values */
   for (uint i = 0; i < n_reductions; i++) {
      if (Op == reduce_op::sum)
         thread_data[i] = 0;
      else
         thread_data[i] = init_val[i];
   }

   /* Check the loop limits and evaluate the loop body */
   if (idx_glob < n_total)
      loop_eval<NDim>(idx_glob, lims, thread_data, loop_body);

   /* Perform reductions */
   for (uint i = 0; i < n_reductions; i++) {
      /* Compute the block-wide sum for thread 0 which stores it */
      if (Op == reduce_op::sum) {
         T aggregate = BlockReduce(temp_storage[i]).Sum(thread_data[i]);
         /* The first thread of each block stores the block-wide aggregate atomically */
         if (threadIdx.x == 0)
            atomicAdd(&rslt[i], aggregate);
      } else if (Op == reduce_op::max) {
         T aggregate = BlockReduce(temp_storage[i]).Reduce(thread_data[i], hipcub::Max());
         if (threadIdx.x == 0)
            atomicMax(&rslt[i], aggregate);
      } else if (Op == reduce_op::min) {
         T aggregate = BlockReduce(temp_storage[i]).Reduce(thread_data[i], hipcub::Min());
         if (threadIdx.x == 0)
            atomicMin(&rslt[i], aggregate);
      } else
         /* Other reduction operations are not supported - print an error message */
         if (threadIdx.x == 0)
            printf("ERROR at %s:%d: Invalid reduction identifier \"Op\".", __FILE__, __LINE__);
   }
}

/* Parallel reduce driver function for the HIP reductions */
template <reduce_op Op, uint NReduStatic, uint NDim, typename Lambda, typename T>
__forceinline__ static void parallel_reduce_driver(const uint (&limits)[NDim], Lambda loop_body, T* sum,
                                                   const uint n_redu_dynamic) {

   /* Get the number of reductions (may be known at compile time or not) */
   const uint n_reductions = NReduStatic ? NReduStatic : n_redu_dynamic;

   /* Calculate the required size for the 1D kernel */
   uint n_total = 1;
   for (uint i = 0; i < NDim; i++)
      n_total *= limits[i];

   /* Check the HIP default mempool settings and correct if wrong */
   device_mempool_check(UINT64_MAX);

   /* Get the CPU thread id */
   const uint thread_id = omp_get_thread_num();

   /* Create a device buffer for the reduction results */
   T* d_buf;
   CHK_ERR(hipMallocAsync(&d_buf, n_reductions * sizeof(T), gpuStreamList[thread_id]));
   CHK_ERR(hipMemcpyAsync(d_buf, sum, n_reductions * sizeof(T), hipMemcpyHostToDevice, gpuStreamList[thread_id]));

   /* Create a device buffer to transfer the initial values to device */
   T* d_const_buf;
   CHK_ERR(hipMallocAsync(&d_const_buf, n_reductions * sizeof(T), gpuStreamList[thread_id]));
   CHK_ERR(
       hipMemcpyAsync(d_const_buf, d_buf, n_reductions * sizeof(T), hipMemcpyDeviceToDevice, gpuStreamList[thread_id]));

   /* Create a device buffer to transfer the loop limits of each dimension to device */
   uint* d_limits;
   CHK_ERR(hipMallocAsync(&d_limits, NDim * sizeof(uint), gpuStreamList[thread_id]));
   CHK_ERR(hipMemcpyAsync(d_limits, limits, NDim * sizeof(uint), hipMemcpyHostToDevice, gpuStreamList[thread_id]));

   /* Call the reduction kernel with different arguments depending
    * on if the number of reductions is known at the compile time
    */
   T* d_thread_data_dynamic = 0; // declared zero to suppress unitialized use warning
   if (NReduStatic == 0) {
      /* Get the cub temp storage sizes for the dynamic shared memory kernel argument */
      constexpr auto cub_temp_storage_type_size =
          sizeof(typename hipcub::BlockReduce<T, ARCH_BLOCKSIZE_R, hipcub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY, 1,
                                              1>::TempStorage);
      constexpr auto cub_temp_storage_type_size_small =
          sizeof(typename hipcub::BlockReduce<T, ARCH_BLOCKSIZE_R_SMALL, hipcub::BLOCK_REDUCE_RAKING_COMMUTATIVE_ONLY,
                                              1, 1>::TempStorage);
      /* Query device properties */
      int device_id;
      CHK_ERR(hipGetDevice(&device_id));
      hipDeviceProp_t deviceProp;
      CHK_ERR(hipGetDeviceProperties(&deviceProp, device_id));
      /* Make sure there is enough shared memory for the used block size */
      uint blocksize;
      size_t shared_mem_bytes_per_block_request;
      if (n_reductions * cub_temp_storage_type_size <= deviceProp.sharedMemPerBlock) {
         blocksize = ARCH_BLOCKSIZE_R;
         shared_mem_bytes_per_block_request = n_reductions * cub_temp_storage_type_size;
      } else if (n_reductions * cub_temp_storage_type_size_small <= deviceProp.sharedMemPerBlock) {
         blocksize = ARCH_BLOCKSIZE_R_SMALL;
         shared_mem_bytes_per_block_request = n_reductions * cub_temp_storage_type_size_small;
      } else {
         printf("The device %d (%s) does not have enough shared memory even for the small blocksize (%d)! The error "
                "occurred in %s at line %d\n",
                device_id, deviceProp.gcnArchName, ARCH_BLOCKSIZE_R_SMALL, __FILE__, __LINE__);
         exit(1);
      }
      /* Set the kernel grid dimensions */
      const uint gridsize = (n_total - 1 + blocksize) / blocksize;
      /* Allocate memory for the thread data values */
      CHK_ERR(hipMallocAsync(&d_thread_data_dynamic, n_reductions * blocksize * gridsize * sizeof(T),
                             gpuStreamList[thread_id]));
      /* Call the kernel (the number of reductions not known at compile time) */
      if (gridsize > 0) {
         if (blocksize == ARCH_BLOCKSIZE_R) {
            reduction_kernel<ARCH_BLOCKSIZE_R, Op, NDim, 0>
                <<<gridsize, blocksize, shared_mem_bytes_per_block_request, gpuStreamList[thread_id]>>>(
                    loop_body, d_const_buf, d_buf, d_limits, n_total, n_reductions, d_thread_data_dynamic);
         } else if (blocksize == ARCH_BLOCKSIZE_R_SMALL) {
            reduction_kernel<ARCH_BLOCKSIZE_R_SMALL, Op, NDim, 0>
                <<<gridsize, blocksize, shared_mem_bytes_per_block_request, gpuStreamList[thread_id]>>>(
                    loop_body, d_const_buf, d_buf, d_limits, n_total, n_reductions, d_thread_data_dynamic);
         } else {
            printf("The blocksize (%u) does not match with any of the predetermined block sizes! The error occurred in "
                   "%s at line %d\n",
                   blocksize, __FILE__, __LINE__);
            exit(1);
         }
      }
      /* Check for kernel launch errors */
      CHK_ERR(hipPeekAtLastError());
      /* Synchronize and free the thread data allocation */
      CHK_ERR(hipStreamSynchronize(gpuStreamList[thread_id]));
      CHK_ERR(hipFreeAsync(d_thread_data_dynamic, gpuStreamList[thread_id]));
   } else {
      /* Set the kernel dimensions */
      const uint blocksize = ARCH_BLOCKSIZE_R;
      const uint gridsize = (n_total - 1 + blocksize) / blocksize;
      /* Call the kernel (the number of reductions known at compile time) */
      if (gridsize > 0)
         reduction_kernel<ARCH_BLOCKSIZE_R, Op, NDim, NReduStatic>
             <<<gridsize, blocksize, 0, gpuStreamList[thread_id]>>>(loop_body, d_const_buf, d_buf, d_limits, n_total,
                                                                    n_reductions, d_thread_data_dynamic);
      /* Check for kernel launch errors */
      CHK_ERR(hipPeekAtLastError());
      /* Synchronize after kernel call */
      CHK_ERR(hipStreamSynchronize(gpuStreamList[thread_id]));
   }
   /* Copy the results back to host and free the allocated memory back to pool*/
   CHK_ERR(hipMemcpyAsync(sum, d_buf, n_reductions * sizeof(T), hipMemcpyDeviceToHost, gpuStreamList[thread_id]));
   CHK_ERR(hipFreeAsync(d_buf, gpuStreamList[thread_id]));
   CHK_ERR(hipFreeAsync(d_const_buf, gpuStreamList[thread_id]));
   CHK_ERR(hipFreeAsync(d_limits, gpuStreamList[thread_id]));
}
} // namespace arch

#endif // !ARCH_DEVICE_HIP_H
